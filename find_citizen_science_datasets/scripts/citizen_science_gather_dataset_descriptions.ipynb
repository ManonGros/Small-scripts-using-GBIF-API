{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import textblob\n",
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we predict if a dataset is part of a citizen science project?\n",
    "\n",
    "With the help of the [GBIF API](https://www.gbif.org/developer/summary) and textblob.\n",
    "\n",
    "See the documentation I used to write this  script:\n",
    "* https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "* https://stevenloria.com/simple-text-classification/\n",
    "\n",
    "## What do I label as citizen science in the training set?\n",
    "\n",
    "### What I label as \"citizen science\":\n",
    "* Metadata explicitly includes the words \"citizen\" or \"citizen science\" (or for the French version \"science\" or \"enquÃªte\" participative)\n",
    "* The metadata mentions that the dataset is partly or entirely made by volunteers\n",
    "* Bioblitz datasets\n",
    "\n",
    "### What I don't label as \"citizen science\":\n",
    "* What seems like compulsory student work\n",
    "* Personal collections or notebook (unless the description includes clue from above)\n",
    "\n",
    "## I - get the dataset's features to analyse from GBIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_set_of_words(dataset):\n",
    "    '''\n",
    "    Puts together titles, descriptions, methods and keywords\n",
    "    '''\n",
    "    set_of_words = dataset[\"title\"] + \" \"\n",
    "    if \"description\" in dataset:\n",
    "        set_of_words += dataset[\"description\"]+ \" \"\n",
    "    # Get keywords\n",
    "    if \"keywordCollections\" in dataset:\n",
    "        for kwcollection in dataset[\"keywordCollections\"]:\n",
    "            if \"keywords\" in kwcollection:\n",
    "                for kw in kwcollection[\"keywords\"]:\n",
    "                    set_of_words += str(kw)+ \" \"\n",
    "    # Get Methods\n",
    "    if \"samplingDescription\" in dataset:\n",
    "        for key in dataset[\"samplingDescription\"]:\n",
    "            if key != \"methodSteps\":\n",
    "                set_of_words += dataset[\"samplingDescription\"][key]+ \" \"\n",
    "            else:\n",
    "                for methodStep in dataset[\"samplingDescription\"][key]:\n",
    "                    set_of_words += str(methodStep)+ \" \"\n",
    "    return set_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=[\"Description\", \"Language\"])\n",
    "\n",
    "# We are excluding datasets from organizations that generated the metadata automatically\n",
    "# PLAZI and PANGEA\n",
    "exclude = ['7ce8aef0-9e92-11dc-8738-b8a03c50a862',\n",
    "           'd5778510-eb28-11da-8629-b8a03c50a862']\n",
    "offset = 0\n",
    "step = 900\n",
    "end_of_records = False\n",
    "while not end_of_records:\n",
    "    param = {\n",
    "        \"offset\": offset,\n",
    "        \"limit\": step\n",
    "    }\n",
    "    # Query API\n",
    "    response = requests.get(\"http://api.gbif.org/v1/dataset\", param)\n",
    "    response = response.json()\n",
    "    offset += step\n",
    "    end_of_records = response[\"endOfRecords\"]\n",
    "    for dataset in response[\"results\"]:\n",
    "        # exclude dataset from PLAZI and PANGEA\n",
    "        if dataset[\"publishingOrganizationKey\"] not in exclude and dataset[\"type\"] != \"CHECKLIST\":\n",
    "            res.at[dataset[\"key\"], \"Language\"] = dataset[\"language\"]\n",
    "            # Get title, description, keywords, methods of the dataset\n",
    "            res.at[dataset[\"key\"], \"Description\"] = extract_set_of_words(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to avoid rerunning everything, everytime I debug my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.to_csv(\"../raw_descriptions.tsv\", index=True, sep=\"\\t\")\n",
    "\n",
    "res = pd.read_table(\"../raw_descriptions.tsv\")\n",
    "res = res.set_index(\"UUID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training  + testing sets: partially manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_table(\"../some_manually_annotated_datasets.tsv\")\n",
    "training_set = training_set.set_index(\"UUID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Set the parameters for data cleaning and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Description\" # we want to find words in the description\n",
    "frequency_threshold = 15 # how many most frequent word do we remove\n",
    "rare_threshold_word_number = 2 # how many time a word should appear to be kept\n",
    "parameter_selection_training_set = 800 # Size of training set + selection of parameters\n",
    "crossValidation = 4\n",
    "triming_threshold_for_model = 15 # number of words we want to keep from model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove underscore\n",
    "res[feature] = res[feature].str.replace(\"_\", \" \")\n",
    "res[feature] = res[feature].str.replace(\"-\", \" \")\n",
    "\n",
    "# Set everything to lower case\n",
    "res[feature] = res[feature].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the translation stopped working after an hour of using it. Apparently iy is a common issue with libraries using the google API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translate sentence if needed\n",
    "# for dataset in res.index.tolist():\n",
    "#     if res.loc[dataset, \"Language\"] != \"eng\":\n",
    "#         print(dataset)\n",
    "#         res.at[dataset, feature] = str(textblob.TextBlob(res.loc[dataset, feature]).translate(to='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "res[feature] = res[feature].str.replace('[^\\w\\s]', '')\n",
    "res[feature] = res[feature].str.replace('[\\d]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequent words by language\n",
    "for language in list(set(res.Language.tolist())):\n",
    "    freq = pd.Series(' '.join(res[res.Language == language][feature]).split()).value_counts()[:frequency_threshold]\n",
    "    freq = list(freq.index)\n",
    "    # Remove the most frequent words\n",
    "    res[feature] = res[feature].apply(lambda x: \" \".join([c for c in x.split() if c not in freq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace some key words\n",
    "words_to_replace = pd.read_table(\"../wd_replace.txt\")\n",
    "words_to_replace = words_to_replace.set_index(\"word\")\n",
    "for word in words_to_replace.index.tolist():\n",
    "    res[feature] = res[feature].str.replace(word, words_to_replace.loc[word, \"replacement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the rare words # OPTIONAL because it takes too much time\n",
    "# freq = pd.Series(' '.join(res[feature]).split()).value_counts()\n",
    "# freq = freq[freq < rare_threshold_word_number]\n",
    "# freq = list(freq.index)\n",
    "# # Remove rare words\n",
    "# res[feature] = res[feature].apply(lambda x: \" \".join([c for c in x.split() if c not in freq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct spelling - not done here because it takes too much time\n",
    "# res[feature] = res[feature].apply(lambda x: str(textblob.TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (remove some letters in words to make it more \"universal\")\n",
    "res[feature] = res[feature].apply(lambda x: \" \".join([textblob.Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# Remove as many NAs as possible\n",
    "res[feature] = res[feature].str.replace(\" na \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "data_training_set = pd.concat([res, training_set], join=\"inner\", axis=1)\n",
    "\n",
    "# Reformat dataset\n",
    "index_list = data_training_set.index.tolist()\n",
    "random.shuffle(index_list)\n",
    "index_test_set = index_list[parameter_selection_training_set:len(index_list)]\n",
    "index_training_set = index_list[0:parameter_selection_training_set]\n",
    "\n",
    "data_training_set = data_training_set.loc[index_training_set]\n",
    "data_training_set = list(data_training_set[[feature, 'CS']].itertuples(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Train and test Classifier - Naive Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "word_for_model = []\n",
    "\n",
    "for fold in range(0, crossValidation):\n",
    "    cl = NaiveBayesClassifier(data_training_set[fold:(fold+1)*int(len(data_training_set)/crossValidation)])\n",
    "    informative_feature = cl.informative_features(triming_threshold_for_model)\n",
    "    for word in informative_feature:\n",
    "        word_for_model.append(word[0].replace(\"contains(\",\"\").replace(\")\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put back the performance in the context \n",
    "training_set[\"CS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Languages\n",
    "res[\"Language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - What word is associated with citizen science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most informative words\n",
    "keywords = pd.Series(word_for_model).value_counts()\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI - Train and test model on reduced set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_set_of_words = keywords[keywords > 1].index.tolist()\n",
    "reduced_set_of_words += [\"herbarium\", \"museum\", \"inaturalist\"]\n",
    "reduced_set_of_words = set(reduced_set_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[feature] = res[feature].apply(lambda x: \" \".join([c for c in x.split() if c in reduced_set_of_words]))\n",
    "res[feature] = res[feature].str.replace(\" na \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced training set\n",
    "reduced_set = pd.concat([res, training_set], join=\"inner\", axis=1)\n",
    "\n",
    "# Reformat dataset\n",
    "reduced_training_set = reduced_set.loc[index_training_set]\n",
    "reduced_training_set = list(reduced_training_set[[feature, 'CS']].itertuples(index=False))\n",
    "\n",
    "test_set = reduced_set.loc[index_test_set]\n",
    "test_set = list(test_set[[feature, 'CS']].itertuples(index=False))\n",
    "\n",
    "# train dataset\n",
    "# cl = NaiveBayesClassifier(reduced_training_set)\n",
    "cl = textblob.classifiers.DecisionTreeClassifier(reduced_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.pseudocode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truePositive = 0\n",
    "trueNegative = 0\n",
    "falsePositive = 0\n",
    "falseNegative = 0\n",
    "for test in test_set:\n",
    "    if test[1] == \"T\":\n",
    "        if cl.classify(test[0]) == \"T\":\n",
    "            truePositive += 1\n",
    "        else:\n",
    "            falsePositive += 1\n",
    "    else:\n",
    "        if cl.classify(test[0]) == \"T\":\n",
    "            falseNegative += 1\n",
    "        else:\n",
    "            trueNegative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance on testing set\\n\")\n",
    "print(\"True positive:\\t\", truePositive*100/len(test_set))\n",
    "print(\"True negative:\\t\", trueNegative*100/len(test_set))\n",
    "print(\"False positive:\\t\", falsePositive*100/len(test_set))\n",
    "print(\"False negative:\\t\", falseNegative*100/len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\")\n",
    "cl.accuracy(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put back the performance in the context \n",
    "reduced_set.loc[index_test_set][\"CS\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the rest of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = pd.DataFrame([\"UUID\", \"CS\"])\n",
    "for test in res.index.tolist():\n",
    "    if test not in training_set.index.tolist():\n",
    "        to_check.at[test, \"CS\"] = cl.classify(res.at[test, feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_and_description = pd.read_table(\"../raw_occurrence_dataset_descriptions_and_titles.tsv\")\n",
    "title_and_description = title_and_description.set_index(\"UUID\")\n",
    "to_check_w_title = pd.concat([title_and_description, to_check], join = \"inner\", axis=1)\n",
    "to_check_w_title.sort_values([\"CS\"], ascending=False).to_csv(\"../test_model_subsample.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
